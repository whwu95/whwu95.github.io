<html>
<head>
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
	<title>Wenhao Wu (吴文灏)</title>
	<meta content="Wenhao Wu (吴文灏), whwu95.github.io" name="keywords" />
	<style media="screen" type="text/css">
  html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: Arial, Helvetica, sans-serif;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}

a {
  color: #043d98;
  text-decoration:none;
}

a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}



* {
  margin: 0pt;
  padding: 0pt;
}

body {
  position: relative;
  margin: 2em auto 2em auto;
  width: 870px;
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 15px;
  background: #F4F6F6;
}

h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15pt;
  font-weight: 700;
}

h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  font-weight: 700;
}

strong {
  /* font-family: Lato, Verdana, Helvetica, sans-serif; */
  /* font-size: 13px; */
  font-weight:bold;
}

ul { 
  /* list-style: circle; */
  list-style: disc;
}

img {
  border: none;
}

li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}

alert {
  font-family: Arial, Helvetica, sans-serif;
  font-size: 14px;
  font-weight: bold;
  color: #FF0000;
}

em, i {
	font-style:italic;
}

div.section {
  clear: both;
  margin-bottom: 1.2em;
  background: #F4F6F6;
}

div.spanner {
  clear: both;
}

div.paper {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 2px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}
div.paper2 {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 140%;
}

div.paper:hover {
    background: #FFFDEE;
    /* background-color: #242d36 ; */
}

div.paper2:hover {
    background: #FFFDEE;
    /* background-color: #242d36 ; */
}
div.bio {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.7em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.55em .8em 0.6em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 135%;
}

div.res {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.award {
  clear: both;
  margin-top: 0.4em;
  margin-bottom: 0.4em;
  border: 0px solid #ddd;
  background: #fff;
  padding: 0.65em .8em 0.15em .8em;
  border-top-right-radius:10px; 
  border-top-left-radius:10px; 
  border-bottom-left-radius:10px; 
  border-bottom-right-radius:10px;
  line-height: 130%;
}

div.paper div {
  padding-left: 270px;
}

img.paper {
  /* margin-bottom: 0.4em; */
  float: left;
  width: 250px;

}

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}

pre, code {
  font-family: Open Sans Light, Helvetica, sans-serif;
  font-size: 14px;
  margin: 1em 0;
  padding: 0;
}

    .bot {
  font-size: 14%;
}

   .ptypej {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #5cb85c;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypec {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #428bca;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
   .ptypep {
    display: inline;
    padding: .0em .2em .05em;
    font-size: 85%;
    font-weight: bold;
    line-height: 1;
  background-color: #6B6B6B;
    color: #FFFFFF;
    text-align: center;
    white-space: nowrap;
    vertical-align: baseline;
  margin-right: 6px;
}
/* navigation */
#nav {
  /* font-family: 'Lucida Grande', 'Lucida Sans Unicode', 'Lucida Sans',*/
       /* Corbel, Arial, Helvetica, sans-serif; */
  font-family: Georgia, Helvetica, sans-serif;
  position: fixed;
  top: 50px;
  /* left: 860px; */
  margin-left: 860px;     /*1060*/
  width: 92px;
  font-size: 15px;
}

#nav li2 {
    margin-bottom: 1px;
}
ol {
  list-style: none;
}
#nav a {
    display: block;
    padding: 6px 9px 7px;
    color: #fff;
    background-color: #455A64;
    text-decoration: none;
}

#nav a:hover {
    color: #ffde00;
    /* background-color: #242d36 ; */
}
</style>

<!-- <script type="text/javascript" async="" src="./files/ga.js"></script>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-7953909-1']);
  _gaq.push(['_trackPageview']);

  (function () {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script> -->

<script type="text/javascript" src="./files/hidebib.js"></script>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');



</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');

</script>

<!-- <script src="./files/main.js"></script> -->

<body>
  <ol id="nav">
    <li><a href="#home" title="Home">Home</a></li>
    <li><a href="#news" title="News">News</a></li>
    <li><a href="#pub" title="Papers">Papers</a></li>
    <li><a href="mailto:wuwenhao17@mails.ucas.edu.cn" title="Contact">Contact</a></li>
  </ol>
<a name="home"></a>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Wenhao Wu (吴文灏)" style="float: left; padding-left: .01em; height: 140px;" src="whwu_fuji.jpg" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Wenhao Wu (吴文灏)</span><br />
<span><strong>Baidu VIS R&D Engineer</strong></span><br /> 
<span>Department of Computer Vision Technology (VIS) </span> <br />
<span>Baidu Inc.</span><br />
<!-- <span><strong>Email  </strong>: wuwenhao17[at]mails[dot]ucas[dot]edu[dot]cn</span> <br />  -->
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<!-- <h2>(<a href='https://scholar.google.com/citations?user=kWADCMUAAAAJ&hl=zh-CN'>Google scholar</a>)</h2> -->
<h2>About Me (<a href='https://www.linkedin.com/in/wenhao-w-aab583128/?locale=en_US'>Linkedin</a>, <a href='https://www.zhihu.com/people/wu-wen-hao-80-23'>知乎</a>)</h2>
<div class="bio">


Currently, I am working at Baidu VIS as a computer vision researcher. Previously, I received my Master Degree from <a href='http://english.ucas.ac.cn/'>University of Chinese Academy of Sciences</a>, advised by Prof. <a href='http://mmlab.siat.ac.cn/sfchen'>Shifeng Chen</a>
and Prof. <a href='http://mmlab.siat.ac.cn/yuqiao/'>Yu Qiao</a> in 07/2020. I received my B.Eng. degree from <a href='http://en.csu.edu.cn/'>Central South University</a> in 06/2017.
I also had a great time researching computer vision at <a href='https://www.sensetime.com/en/'>SenseTime Research</a>, 
<a href='http://research.baidu.com'>Baidu Research</a>, <a href='https://ir.iqiyi.com/investor-overview'>iQIYI</a>, 
<a href='https://research.samsung.com/src-b'>Samsung Research China</a> and 
<a href='http://mmlab.siat.ac.cn/team'>MMLAB of CUHK at Shenzhen</a>.

</div>
</div>


<a name="news"></a>
<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
    <!-- <li> <alert>I will graduate in 2020 fall and I am now looking for a Ph.D. program. Please feel free to contact me.</alert> </li> -->
    <!-- <li> <alert> I will be a researcher at Tencent Youtu X-lab. Thrilled to start a new journey. </li> -->
    <li> 2021.04: One paper accepted by <font color="DarkRed">IJCAI2021</font>.</li>
    <li> 2021.04: We <font color="Red">rank first</font> in the Traffic Anomaly detection Track of the <a href="https://www.aicitychallenge.org/">CVPR 2021 AI CITY CHALLENGE</a>.</li>
    <li> 2020.12: One paper accepted by <font color="DarkRed">AAAI2021</font>.</li>
    <li> 2020.07: One paper accepted by <font color="DarkRed">ECCV2020</font>.</li>
    <li> 2020.05: One paper was accepted for <b>Oral presentation</b> on <font color="Green">CVPR2020 EDLCV workshop</font>.</li>
    <!-- <li> 2020.01: Joined BigVideo Team at <a href='https://www.sensetime.com/en/'>SenseTime Research</a> as a research intern. </li>     -->
    <li> 2019.07: My first paper has been accepted for <font color="Red">Oral presentation</font> on <font color="DarkRed">ICCV2019</font>. </li>
    <li> 2017.09: Recommended to <a href='http://english.ucas.ac.cn/'>University of Chinese Academy of Sciences</a> towards the MSc degree. </li>	    
    <li> 2017.06: Graduated from Central South University with the outstanding graduate honor. </li>
    <li> 2016.10: Joined <a href='http://mmlab.siat.ac.cn/team'>MMLAB of the Chinese University of Hong Kong at Shenzhen</a> as research intern. Started doing research on Computer Vision. </li>

    </ul>
  </div>
</div>
</div>

  <div style="clear: both;">
    <div class="section">
      <h2 id="confpapers">Industrial Experience</h2>
      <div class="paper">
        <ul>
          <li>
            Jul. 2020 - Present, Senior R&D Engineer, <font color="Brown">Baidu VIS</font>, hosted by <a
              href="https://scholar.google.com/citations?hl=zh-CN&user=1wzEtxcAAAAJ">Errui Ding</a>
          </li>

          <li>
            Jan. 2020 - Feb. 2020, Research Intern, <font color="Brown">SenseTime Research</font> - BigVideo Team, hosted by <a
              href="https://scholar.google.com/citations?user=eGD0b7IAAAAj">Kai Chen</a>
          </li>

          <li>
            Oct. 2018 - Jan. 2020, Research Intern, <font color="Brown">Baidu Research</font> & <font color="Brown">Baidu VIS</font>, hosted by <a
              href="https://scholar.google.com/citations?hl=zh-CN&user=zKtYrHYAAAAJ">Shilei Wen</a> and <a
              href="https://scholar.google.com/citations?hl=zh-CN&user=1wzEtxcAAAAJ">Errui Ding</a>
          </li>

          <li>
            Jun. 2018 - Oct. 2018, Research Intern, <font color="Brown">iQIYI</font> - Video Analysis Group, hosted by </a>Qiyue Liu</a>
          </li>
  
          <li>
            Mar. 2018 - Jun. 2018, Research Intern, <font color="Brown">Samsung Research China</font> - Machine Learning Lab, hosted by <a
              href="https://www.linkedin.com/in/%E7%BD%97%E6%8C%AF%E6%B3%A2-07b971166/">Zhenbo luo</a>
          </li>
  
        </ul>
  
        <div class="spanner"></div>
      </div>
    </div>
  </div>





<a name="pub"></a>
<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Publications (* = co-first author)</h2>

<!-- under review -->
<div class="paper" id="xxx"><img class="paper" src="papers/Arxiv/ASCNet.png" />
  <div>
    <a><b>ASCNet: Self-supervised Video Representation Learning with Appearance-Speed Consistency</b></a><br />
    Deng Huang*, <u><b style="color:darkred">Wenhao Wu*</b></u>, Weiwen Hu, Xu Liu, Dongliang He, Zhihua Wu, Xiangmiao Wu, Mingkui Tan, Errui Ding <br />
    Technical Report <br />
    [ <a href='https://arxiv.org/pdf/2106.02342.pdf'>PDF</a> ]
    [ <a href=''>Code</a> ] <br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>


<!-- under review -->
<div class="paper" id="xxx"><img class="paper" src="papers/Arxiv/DSANet.png" />
  <div>
    <a><b>DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning</b></a><br />
    <u><b style="color:darkred">Wenhao Wu*</b></u>, Yuxiang Zhao*, Yanwu Xu, Xiao Tan, Dongliang He, Zhikang Zou, Jin Ye, Yingying Li, Mingde Yao, Zichao Dong,
    Yifeng Shi <br />
    Technical Report <br />
    [ <a href='https://arxiv.org/pdf/2105.12085.pdf'>PDF</a> ]
    [ <a href=''>Code</a> ] <br />
    <alert>An efficient plug-and-play module for effective video-level representation learning.</alert>
  </div>
  <div class="spanner"></div>
</div>

<!-- under review -->
<div class="paper" id="xxx"><img class="paper" src="papers/Arxiv/color2style.png" />
  <div>
    <a><b>Color2Style: Real-Time Exemplar-Based Image Colorization with Self-Reference Learning and Deep Feature Modulation</b></a><br />
    Henyuan Zhao*, <u><b style="color:darkred">Wenhao Wu*</b></u>, Yihao Liu*, Dongliang He <br />
    Technical Report <br />
    [ <a href='https://arxiv.org/pdf/2106.08017.pdf'>PDF</a> ]
    [ <a href='https://github.com/zhaohengyuan1/Color2Style'>Code</a> ] <br />
    <!-- <alert>An effective self-supervised video representation learning framework.</alert> -->
  </div>
  <div class="spanner"></div>
</div>


<!-- under review -->
<div class="paper" id="xxx"><img class="paper" src="papers/Arxiv/TAPG-TR.png" />
  <div>
    <a><b>Temporal Action Proposal Generation with Transformers</b></a><br />
    Lining Wang*, Haosen Yang*, <u><b style="color:darkred">Wenhao Wu*</b></u>, Hongxun Yao, Hujie Huang <br />
    Technical Report <br />
    [ <a href='https://arxiv.org/pdf/2105.12043.pdf'>PDF</a> ]
    [ <a href=''>Code</a> ] 
    <br />
    <alert>Transformers for temporal action proposal generation with obtaining SOTA performance.</alert>
  </div>
  <div class="spanner"></div>
</div>

<!-- IJCAI2021 -->
<div class="paper" id="IJCAI2021"><img class="paper" src="papers/IJCAI2021/WSSTAD.png" />
  <div>
    <a><b>Weakly-Supervised Spatio-Temporal Anomaly Detection in Surveillance Video</b></a><br />
    Jie Wu, Wei Zhang, Guanbin Li, <u><b style="color:darkred">Wenhao Wu</b></u>, Xiao Tan, Yingying Li, Errui Ding, Liang Lin <br />
    <i> Joint Conference on Artificial Intelligence <b><font color="DarkRed">(IJCAI)</font></b>, 2021</i>  <br />

    [ <a href='papers/IJCAI2021/IJCAI2021_WSSTAD.pdf'>PDF</a> ]
    [ <a href=''>Code</a> ] <br />

    <alert>We introduce a novel task: Weakly-Supervised Spatio-Temporal Anomaly Detection.</alert>
  </div>
  <div class="spanner"></div>
</div>

<!-- CVPRW2021 -->
<div class="paper" id="CVPRW2021"><img class="paper" src="papers/CVPRW2021/aicity.png" />
<div>
  <a><b>Good Practices and A Strong Baseline for Traffic Anomaly Detection</b></a><br />
    Yuxiang Zhao*, <u><b style="color:darkred">Wenhao Wu*</b></u>, Yue He, Yingying Li, Xiao Tan, Shifeng Chen <br />

  <i>IEEE Conference on Computer Vision and Pattern Recognition <b><font color="DarkRed">(CVPR)</font></b> - 5th AI City Challenge (AICity), 2021</i> <br /> 

    [ <a href='https://arxiv.org/abs/2105.03827'>PDF</a> ]
    [ <a href=''>Code</a> ]
    [ <a shape="rect" href="javascript:togglebib(&#39;CVPRW2021&#39;)" class="togglebib">Bibtex</a> ]<br />
  <pre xml:space="preserve" style="display: none;">
@inproceedings{zhao2021good,
title={Good Practices and A Strong Baseline for Traffic Anomaly Detection},
author={Zhao, Yuxiang and Wu, Wenhao and He, Yue and Li, Yingying and Tan, Xiao and Chen, Shifeng},
booktitle={Proceedings of CVPR Workshops},
year={2021}
}
  </pre>
  <alert>Winner of AI City challenge for traffic anomaly detection</alert>
</div>
<div class="spanner"></div>
</div>


<!-- AAAI2021 -->
<div class="paper" id="AAAI2021"><img class="paper" src="papers/AAAI2021/MVF.png" />
  <div>
    <a><b>MVFNet: Multi-View Fusion Network for Efficient Video Recognition</b></a><br />
      <u><b style="color:darkred">Wenhao Wu</b></u>, Dongliang He, Tianwei Lin, Fu Li, Chuang Gan, Errui Ding <br />
    <i>The AAAI Conference on Artificial Intelligence <b><font color="DarkRed">(AAAI)</font></b>, 2021</i> <br />
      <b><font color="Green">[21% acceptance rate]</font></b> 
      [ <a href='https://arxiv.org/pdf/2012.06977.pdf'>PDF</a> ]
      [ <a href='https://github.com/whwu95/MVFNet'>Code</a> ]
      [ <a shape="rect" href="javascript:togglebib(&#39;AAAI2021&#39;)" class="togglebib">Bibtex</a> ]<br />
    <pre xml:space="preserve" style="display: none;">
@inproceedings{wu2020MVFNet,
author = {Wu, Wenhao and He, Dongliang and Lin, Tianwei and Li, Fu and Gan, Chuang and Ding, Errui},
title = {MVFNet: Multi-View Fusion Network for Efficient Video Recognition},
booktitle = {AAAI},
year = {2021}
}
    </pre>
    <alert>An efficient architecture for video recognition based on 2D CNN.</alert>
  </div>
  <div class="spanner"></div>
</div>


<!-- ECCV2020 -->
<div class="paper" id="ECCV2020"><img class="paper" src="papers/ECCV2020/arch.png" />
  <div>
    <a><b>Attention-Driven Dynamic Graph Convolutional Network for Multi-Label Image Recognition</b></a><br />
      Jin Ye, Junjun He, Xiaojiang Peng, <u><b style="color:darkred">Wenhao Wu</b></u>, Yu Qiao <br />
    <i>European Conference on Computer Vision <b><font color="DarkRed">(ECCV)</font></b>, 2020 </i> <br />
      <b><font color="Green">[Poster]</font></b>
      [ <a href='http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660647.pdf'>PDF</a> ]
      [ <a shape="rect" href="javascript:togglebib(&#39;ECCV2020&#39;)" class="togglebib">Bibtex</a> ]<br />
    <pre xml:space="preserve" style="display: none;">
@inproceedings{ADD-GCN,
    title={Attention-Driven Dynamic Graph Convolutional Network for Multi-label Image Recognition},
    author={Ye, Jin and He, Junjun and Peng, Xiaojiang and Wu, Wenhao and Qiao, Yu}
    booktitle={Proceedings of ECCV 2020},
    pages={649--665},
    year={2020}
}
    </pre>
  </div>
  <div class="spanner"></div>
</div>



<!-- CVPRW2020 -->
<div class="paper" id="CVPRW2020"><img class="paper" src="papers/CVPRW2020/teaser.gif" />
<div>
  <a><b>Dynamic Inference: A New Approach Toward Efficient Video Action Recognition</b></a><br />
    <u><b style="color:darkred">Wenhao Wu</b></u>, Dongliang He, Xiao Tan, Shifeng Chen, Yi Yang, Shilei Wen <br />
  <i>IEEE Conference on Computer Vision and Pattern Recognition <b><font color="DarkRed">(CVPR)</font></b> - Joint Workshop on
  Efficient Deep Learning in Computer Vision (EDLCV), 2020 </i><br />

    <b><font color="Red">[Oral]</font></b>
    [ <a
    href='http://openaccess.thecvf.com/content_CVPRW_2020/html/w40/Wu_Dynamic_Inference_A_New_Approach_Toward_Efficient_Video_Action_Recognition_CVPRW_2020_paper.html'>PDF</a> ]
    [ <a href='papers/CVPRW2020/CVPR20_EDLCV_Oral_15min.pdf'>Slides</a> ]
    [ <a shape="rect" href="javascript:togglebib(&#39;CVPRW2020&#39;)" class="togglebib">Bibtex</a> ]<br />
    <pre xml:space="preserve" style="display: none;">  
@inproceedings{wu2020dynamic,
    title={Dynamic Inference: A New Approach Toward Efficient Video Action Recognition},
    author={Wu, Wenhao and He, Dongliang and Tan, Xiao and Chen, Shifeng 
      and Yang, Yi and Wen, Shilei},
    booktitle={Proceedings of CVPR Workshops},
    pages={676--677},
    year={2020}
}
    </pre>
</div>
<div class="spanner"></div>
</div>


<!-- ICCV2019 -->
<div class="paper" id="ICCV19"><img class="paper" src="papers/ICCV2019/ICCV2019.png" />
<div> <b><a>Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition</a></b><br />
<u><b style="color:darkred">Wenhao Wu</b></u>, Dongliang He, Xiao Tan, Shifeng Chen, Shilei Wen <br />
<i>IEEE International Conference on Computer Vision <b><font color="DarkRed">(ICCV)</font></b>, 2019 </i><br />
<b><font color="Red">[Oral, 4.3% acceptance rate]</font></b>
[ <a href='http://openaccess.thecvf.com/content_ICCV_2019/papers/Wu_Multi-Agent_Reinforcement_Learning_Based_Frame_Sampling_for_Effective_Untrimmed_Video_ICCV_2019_paper.pdf'>PDF</a> ]
[ <a href="papers/ICCV2019/MARL_ICCV19_Poster_Wenhao_Wu.pdf">Poster</a> ]
[ <a href="papers/ICCV2019/ICCV19_Oral_5min.pdf">Slides</a> ]
[ <a shape="rect" href="javascript:togglebib(&#39;ICCV19&#39;)" class="togglebib">Bibtex</a> ]<br />
<pre xml:space="preserve" style="display: none;">
@inproceedings{wu2019multi,
    title={Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed
       Video Recognition},
    author={Wu, Wenhao and He, Dongliang and Tan, Xiao and Chen, Shifeng and Wen, Shilei},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    pages={6222--6231},
    year={2019}
}
</pre>
</div>
<div class="spanner"></div>
</div>





<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Contests</h2>
<div class="paper">
<ul>
<!-- <li>The NTIRE Perceptual Extreme Super-Resolution Challenge on CVPR'20: Won 1st in SSIM, 2nd in PSNR, and 11th in LPIPS.</li> -->
<li>Traffic Anomaly detection Track of the CVPR2021 AI CITY CHALLENGE, Rank: 1, 2021</li>
<!-- <li>The ActivityNet Challenge 2018 at CVPR'18: Trimmed Action Recognition (Kinetics-600), Rank: 8</li> -->
<li>The First-class Prize of America Mathematical Contest in Modeling (MCM), 2016 </li>
<li>The First-class Prize in National Undergraduate Mechanical Innovation Design Competition, 2016 </li>
<li>The Second-class Prize in China Freescale Cup Intelligent Car Competition (South China Region), 2015 </li>
<li>The Second-class Prize in Smart Car Racing Competition of Hunan Province, 2015</li>



</ul>
<div class="spanner"></div>
</div>
</div>
</div>







<div style="clear: both;">
<div class="section"><h2>Awards</h2>
<div class="paper">
<li> Excellent Student Cadre of University of Chinese Academy of Sciences, 2020
<li> The Most Outstanding Intern in Baidu (only 2 recipient in the Department), 2019 </li>
<li> Scholarship for Academic Excellence of Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, 2018 </li>
<li> Excellent Undergraduate Student of Central South University, 2017 </li>
<li> Outstanding Student of Central South University, 2016 </li>
<li> National Endeavor Scholarship, 2016 </li>
<li> Excellent Student Cadre of Central South University, 2015</li>
<li> Excellent League Member of Central South University, 2015</li>
<li> Scholarship for Academic Excellence of Central South University, 2014/2015/2016</li>

</div>
</div>
</div>

	
<div style="clear: both;">
<div class="section"><h2>Academic Activities</h2>
<div class="paper"><h3>Presentation</h3> <br>
<li> CVPRW 2020 Oral presentation</li>
<li> ICCV 2019 Oral presentation</li>


<h3>Reviewer</h3> <br>
<li> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
<li> Association for the Advancement of Artificial Intelligence (AAAI), 2021</li>
<li> IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021</li>
</div>
</div>
</div>	


<div style="clear: both;">
<div class="section"><h2>Friends</h2>
<div class="paper">
<a href='https://www.linkedin.com/in/xiao-tan-46b70a85/'>Xiao Tan</a> (Baidu),
<a href='https://www.linkedin.com/in/dongliang-he-6b926077'>Dongliang He</a> (Baidu),
<a href='https://xuyanwu.github.io/'>Yanwu Xu</a> (PITT),
<a href='https://wzmsltw.github.io/'>Tianwei Lin</a> (Baidu),
<a href='https://wujie1010.github.io/'>Jie Wu</a> (Bytedance),
<a href='https://yejin0111.github.io/'>Jin Ye</a> (SIAT),
<a href='https://junjun2016.github.io/'>Junjun He</a> (SJTU)

</div>
</div>
</div>


<div style="clear:both;">
<p align="right"><font size="5">Last Updated on 18st May, 2021</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div>

<hr>
<div id="clustrmaps-widget"></div><script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=0e1633&w=300&t=tt&d=N0ZXDEaZVrn2LXkG_byNAa2NLm2v6WRQIUifhg-2f1A&co=0b4975&ct=cdd4d9&cmo=3acc3a&cmn=ff5353'></script>
<!-- 	<script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=N0ZXDEaZVrn2LXkG_byNAa2NLm2v6WRQIUifhg-2f1A"></script> -->

</body>
</html>
